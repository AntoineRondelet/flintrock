#!/usr/bin/env python3

"""
Nothing here should be distribution-specific (e.g. yum vs. apt-get).
That stuff belongs under image-build/.
"""

import sys
import shlex
import subprocess
import pprint
import asyncio
import functools
import boto.ec2 as ec2
import paramiko
import socket
import json
import yaml
import time
import urllib.request
import tempfile
from datetime import datetime
from collections import namedtuple


SUPPORTED_MODULES = {
    "spark": {
        "version": "1.3.0",
        "git-repo": "https://github.com/apache/spark"
    }
}


def generate_ssh_key_pair() -> namedtuple('KeyPair', ['public', 'private']):
    """
    Generate an SSH key pair that the cluster can use for intra-cluster
    communication.
    """
    with tempfile.TemporaryDirectory() as tempdir:
        ret = subprocess.check_call(
            """
            ssh-keygen -q -t rsa -N '' -f {key_file} -C flintrock
            """.format(
                key_file=shlex.quote(tempdir + "/flintrock_rsa")),
            shell=True)

        with open(file=tempdir + "/flintrock_rsa") as private_key_file:
            private_key = private_key_file.read()

        with open(file=tempdir + "/flintrock_rsa.pub") as public_key_file:
            public_key = public_key_file.read()

    return namedtuple('KeyPair', ['public', 'private'])(public_key, private_key)


def launch(provider, cluster_name, num_slaves, modules, provider_options):
    if provider == 'ec2':
        return launch_ec2(
            cluster_name=cluster_name,
            num_slaves=num_slaves,
            modules=modules,
            **provider_options)
    pass


ClusterInfo = namedtuple(
    'ClusterInfo', [
        'key_pair',
        'master_host',
        'slave_hosts'
    ])

# Combo of
#   * node-specific info
#   * cluster-wide info
# install_spark gets cluster info and specific node to work on
# Module-specific.
# Run from flintrock directly on nodes.
class Spark:
    def __init__(
        self,
        version):
        self.version = version

    def install(self,
        ssh_client: paramiko.client.SSHClient,
        cluster_info: ClusterInfo):
        """
        Runs independently on nodes.
        """
        print("[{h}] Installing Spark...".format(
            h=ssh_client.get_transport().getpeername()[0]))

        with ssh_client.open_sftp() as sftp_client:
            sftp_client.put(
                localpath="./install-spark.sh",
                remotepath="/tmp/install-spark.sh")
            sftp_client.chmod(
                path="/tmp/install-spark.sh",
                mode=0o755)

        stdin, stdout, stderr = ssh_client.exec_command(
            get_pty=True,
            command="""
                /tmp/install-spark.sh {v} hadoop1
            """.format(v=self.version))
        exit_status = stdout.channel.recv_exit_status()
        if exit_status:
            error_output = stderr.read().decode("utf8").rstrip('\n')
            raise Exception(error_output)

        # Pass in a dictionary of all template variables and use
        # that to fill in any number of templates. (?)

        template_path = "/spark/conf/spark-env.sh"
        with open("templates" + template_path) as f:
            spark_env = f.read().format(
                spark_local_dirs="/mnt/spark",
                spark_master_opts="",
                spark_worker_instances="1",
                # spark_worker_cores=num_cores,
                active_master=cluster_info.master_host)

        ssh_command_output(client=ssh_client, command="""
            echo {f} > .{p}
        """.format(
            f=shlex.quote(spark_env),
            p=shlex.quote(template_path)))

    def configure(self):
        """
        Runs after all nodes are "ready".
        """
        pass

    def configure_master(self,
        ssh_client: paramiko.client.SSHClient,
        cluster_info: ClusterInfo):
        print("[{h}] Configuring Spark master...".format(
            h=ssh_client.get_transport().getpeername()[0]))

        stdin, stdout, stderr = ssh_client.exec_command(
            command="""
                set -e
                echo {s} > spark/conf/slaves
                spark/sbin/start-master.sh
                sleep 15
                spark/sbin/start-slaves.sh
            """.format(s=shlex.quote('\n'.join(cluster_info.slave_hosts))))
        # print(stdout.read().decode("utf8"), end='')
        exit_status = stdout.channel.recv_exit_status()
        if exit_status:
            error_output = stderr.read().decode("utf8").rstrip('\n')
            raise Exception(error_output)

    def configure_slave(self):
        pass


# Move to ec2 module and call as ec2.launch()?
def launch_ec2(cluster_name, num_slaves, modules,
    key_name, identity_file,
    instance_type,
    region,
    availability_zone="",
    ami="",
    spot_price=None,
    vpc_id="", subnet_id="", placement_group="",
    tenancy="default", ebs_optimized=False,
    instance_initiated_shutdown_behavior="stop"):
    connection = ec2.connect_to_region(region_name=region)
    def get_or_create_security_group(name, vpc_id):
        search_results = connection.get_all_security_groups(filters={"group-name": [name]})
        group = search_results[0] if search_results else None

        if not group:
            group = connection.create_security_group(
                name=name,
                description="flintrock-managed group",
                vpc_id=vpc_id)

        try:
            # namedtuple('SecurityGroupRule', ['ip_protocol', 'from_port', 'to_port', 'src_group'])(ip_protocol, from_port, to_port, src_group)
            group.authorize(
                ip_protocol='icmp',
                from_port=-1,
                to_port=-1,
                src_group=group)
            group.authorize(
                ip_protocol='tcp',
                from_port=0,
                to_port=65535,
                src_group=group)
            group.authorize(
                ip_protocol='udp',
                from_port=0,
                to_port=65535,
                src_group=group)
        except Exception as e:
            print("ERROR: Make a clean way to check for existing SG rules.")

        flintrock_client_ip = urllib.request.urlopen('http://checkip.amazonaws.com/').read().decode('utf-8').strip()
        flintrock_client_cidr = '{ip}/32'.format(ip=flintrock_client_ip)

        # ssh_rule = None

        # for rule in group.rules:
        #     if int(rule.from_port) == 22 and int(rule.to_port) == 22:
        #         ssh_rule = rule
        #         break

        # if not ssh_rule or flintrock_client_cidr not in [str(grant) for grant in ssh_rule.grants]:
        try:
            group.authorize(
                ip_protocol='tcp',
                from_port=22,
                to_port=22,
                cidr_ip=flintrock_client_cidr)
            group.authorize(
                ip_protocol='tcp',
                from_port=8080,
                to_port=8081,
                cidr_ip=flintrock_client_cidr)
            group.authorize(
                ip_protocol='tcp',
                from_port=4040,
                to_port=4040,
                cidr_ip=flintrock_client_cidr)
        except Exception as e:
            print("ERROR: Really, make it easy to manipulate SG rules.")

        return group
    security_group = get_or_create_security_group(name=cluster_name, vpc_id=vpc_id)
    reservation = connection.run_instances(
        image_id=ami,
        min_count=(num_slaves + 1),
        max_count=(num_slaves + 1),
        key_name=key_name,
        instance_type=instance_type,
        placement=availability_zone,
        security_group_ids=[security_group.id],
        subnet_id=subnet_id,
        placement_group=placement_group,
        tenancy=tenancy,
        ebs_optimized=ebs_optimized,
        instance_initiated_shutdown_behavior=instance_initiated_shutdown_behavior)

    time.sleep(10)  # Eventual consistency complexity tax.

    try:
        while True:
            for instance in reservation.instances:
                if instance.state == 'running':
                    continue
                else:
                    instance.update()
                    time.sleep(3)
                    break
            else:
                print("All {c} instances now running.".format(
                    c=len(reservation.instances)))
                break

        master_instance = reservation.instances[0]
        slave_instances = reservation.instances[1:]

        cluster_info = ClusterInfo(
            key_pair=generate_ssh_key_pair(),
            master_host=master_instance.public_dns_name,
            slave_hosts=[instance.public_dns_name for instance in slave_instances])

        # TODO: Abstract away. No-one wants to see this async shite here.
        loop = asyncio.get_event_loop()

        tasks = []
        for instance in reservation.instances:
            task = loop.run_in_executor(
                executor=None,
                callback=functools.partial(
                    provision_ec2_node,
                    modules=modules,
                    region=region,
                    host=instance.ip_address,
                    identity_file=identity_file,
                    cluster_info=cluster_info))
            tasks.append(task)
        loop.run_until_complete(asyncio.wait(tasks))
        loop.close()

        print("All {c} instances provisioned.".format(
            c=len(reservation.instances)))

        # --- This stuff here runs after all the nodes are provisioned. ---
        with paramiko.client.SSHClient() as client:
            client.load_system_host_keys()
            client.set_missing_host_key_policy(paramiko.client.AutoAddPolicy())

            client.connect(
                username="ec2-user",
                hostname=master_instance.public_dns_name,
                key_filename=identity_file,
                timeout=3)

            for module in modules:
                module.configure_master(
                    ssh_client=client,
                    cluster_info=cluster_info)

            # Run basic Spark health check.
            with open("./spark-health.sh") as health_check:
                stdin, stdout, stderr = client.exec_command(
                    command=health_check.read())
            print(stdout.read().decode("utf8"), end='')
            exit_status = stdout.channel.recv_exit_status()
            if exit_status:
                error_output = stderr.read().decode("utf8").rstrip('\n')
                raise Exception(error_output)

        # Login to the master for manual inspection.
        # TODO: Move to master_login() method.
        # ret = subprocess.call(
        #     """
        #     set -x
        #     ssh -o "StrictHostKeyChecking=no" \
        #         -i {identity_file} \
        #         ec2-user@{host}
        #     """.format(
        #         identity_file=shlex.quote(identity_file),
        #         host=shlex.quote(master_instance.public_dns_name)),
        #     shell=True)
        # print("Shell returned: {r}".format(r=ret))

    except KeyboardInterrupt as e:
        print("Exiting...")
        sys.exit(1)
    finally:
        print("Terminating all {c} instances...".format(
            c=len(reservation.instances)))

        for instance in reservation.instances:
            instance.terminate()


# boto is not thread-safe so each task needs to create its own connection.
# Reference, from boto's primary maintainer: http://stackoverflow.com/a/19542645/
# TODO: Pass in cluster_info object instead of key_pair and master_hostname.
def provision_ec2_node(
    modules,
    region,
    host,
    identity_file,
    cluster_info):
    connection = ec2.connect_to_region(region_name=region)

    with paramiko.client.SSHClient() as client:
        client.load_system_host_keys()
        client.set_missing_host_key_policy(paramiko.client.AutoAddPolicy())

        while True:
            try:
                client.connect(
                    username="ec2-user",
                    hostname=host,
                    key_filename=identity_file,
                    timeout=3)
                print("[{h}] SSH online.".format(h=host))
                break
            except socket.timeout as e:
                time.sleep(5)
            except socket.error as e:
                if e.errno != 61:
                    raise
                time.sleep(5)

        # --- SSH is now available. ---
        stdin, stdout, stderr = client.exec_command(
            command="""
                set -e

                echo {private_key} > ~/.ssh/id_rsa
                echo {public_key} >> ~/.ssh/authorized_keys

                chmod 400 ~/.ssh/id_rsa
            """.format(
                private_key=shlex.quote(cluster_info.key_pair.private),
                public_key=shlex.quote(cluster_info.key_pair.public)))
        exit_status = stdout.channel.recv_exit_status()
        if exit_status:
            error_output = stderr.read().decode("utf8").rstrip('\n')
            raise Exception(error_output)

        # --- Install Modules. ---
        for module in modules:
            module.install(
                ssh_client=client,
                cluster_info=cluster_info)


def ssh_command_output(client: "paramiko.client.SSHClient", command):
    stdin, stdout, stderr = client.exec_command(command, get_pty=True)
    exit_status = stdout.channel.recv_exit_status()
    if exit_status:
        raise Exception(stderr.read().decode("utf8"))

    return stdout.read().decode("utf8").rstrip('\n')


def destroy(provider, cluster_name, provider_options, assume_yes=False):
    pass


def destroy_ec2(cluster_name, assume_yes=False,
    delete_groups=False):
    pass


def add_slaves(provider, cluster_name, num_slaves, provider_options):
    pass


def add_slaves_ec2(cluster_name, num_slaves,
    identity_file):
    pass


def remove_slaves(provider, cluster_name, num_slaves, provider_options, assume_yes=False):
    pass


def remove_slaves_ec2(cluster_name, num_slaves, assume_yes=False):
    pass


def describe(provider, cluster_name, provider_options, master_hostname_only=False):
    pass


def describe_ec2(cluster_name, master_hostname_only=False):
    pass


def login(provider, cluster_name, provider_options):
    pass


def login_ec2(cluster_name):
    pass


def start(provider, cluster_name, provider_options):
    pass


def start_ec2(cluster_name):
    pass


def stop(provider, cluster_name, provider_options, assume_yes=False):
    pass


def stop_ec2(cluster_name, assume_yes=False):
    pass


if __name__ == "__main__":
    # sys.exit()

    spark = Spark(version="1.3.0")
    modules = [spark]

    launch(
        provider="ec2",
        cluster_name="flintrock-test",
        num_slaves=2,
        modules=modules,
        provider_options={
            "key_name": "nick",
            "identity_file": "/Users/nicholaschammas/.ssh/nick.pem",
            "instance_type": "m3.medium",
            "region": "us-east-1",
            "ami": "ami-146e2a7c"
        })
