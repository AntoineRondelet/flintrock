#!/usr/bin/env python3

import sys
import shlex
import subprocess
import pprint
import asyncio
import functools
import boto.ec2 as ec2
import paramiko
import socket
import json
import yaml
import time
import urllib.request
import tempfile
from datetime import datetime
from collections import namedtuple


SUPPORTED_MODULES = {
    "spark": {
        "version": "1.3.0",
        "git-repo": "https://github.com/apache/spark"
    }
}


def generate_ssh_key_pair():
    """
    
    """
    with tempfile.TemporaryDirectory() as tempdir:
        ret = subprocess.check_call(
            """
            # set -x
            ssh-keygen -q -t rsa -N '' -f {key_file} -C flintrock
            # ls -l {tempdir}
            # cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
            """.format(
                key_file=shlex.quote(tempdir + "/flintrock_rsa"),
                tempdir=shlex.quote(tempdir)),
            shell=True)

        with open(file=tempdir + "/flintrock_rsa") as private_key_file:
            private_key = private_key_file.read()

        with open(file=tempdir + "/flintrock_rsa.pub") as public_key_file:
            public_key = public_key_file.read()

    return namedtuple('KeyPair', ['public', 'private'])(public_key, private_key)


def launch(provider, cluster_name, num_slaves, modules, provider_options):
    if provider == 'ec2':
        return launch_ec2(
            cluster_name=cluster_name,
            num_slaves=num_slaves,
            modules=modules,
            **provider_options)
    pass


# Combo of
#   * node-specific info
#   * cluster-wide info
# install_spark gets cluster info and specific node to work on
# Module-specific.
# Run from flintrock directly on nodes.
class Spark:
    def __init__(
        self,
        version,
        ssh_client: "paramiko.client.SSHClient",
        master_hostname=None,
        slave_hostnames=None):
        self.version = version
        self.ssh_client = ssh_client
        self.master_hostname = master_hostname
        self.slave_hostnames = slave_hostnames

    def install(self):
        """
        Runs independently on nodes.
        """
        with self.ssh_client.open_sftp() as sftp_client:
            sftp_client.put(
                localpath="./install-spark.sh",
                remotepath="/tmp/install-spark.sh")
            sftp_client.chmod(
                path="/tmp/install-spark.sh",
                mode=0o755)

        stdin, stdout, stderr = self.ssh_client.exec_command(
            get_pty=True,
            command="""
                /tmp/install-spark.sh {v} hadoop1
            """.format(v=self.version))
        exit_status = stdout.channel.recv_exit_status()
        if exit_status:
            error_output = stderr.read().decode("utf8").rstrip('\n')
            raise Exception(error_output)

        template_path = "/spark/conf/spark-env.sh"
        with open("templates" + template_path) as f:
            spark_env = f.read().format(
                spark_local_dirs="/mnt/spark",
                spark_master_opts="",
                spark_worker_instances="1",
                # spark_worker_cores=num_cores,
                active_master=self.master_hostname)

        ssh_command_output(client=self.ssh_client, command="""
            echo {f} > .{p}
        """.format(
            f=shlex.quote(spark_env),
            p=shlex.quote(template_path)))

    def configure(self):
        """
        Runs after all nodes are "ready".
        """
        pass

    def configure_master(self):
        stdin, stdout, stderr = self.ssh_client.exec_command(
            command="""
                set -e
                echo {s} > spark/conf/slaves
                spark/sbin/start-master.sh
                sleep 15
                spark/sbin/start-slaves.sh
            """.format(s=shlex.quote('\n'.join(self.slave_hostnames))))
        # print(stdout.read().decode("utf8"), end='')
        exit_status = stdout.channel.recv_exit_status()
        if exit_status:
            error_output = stderr.read().decode("utf8").rstrip('\n')
            raise Exception(error_output)

    def configure_slave(self):
        pass


# Move to ec2 module and call as ec2.launch()?
def launch_ec2(cluster_name, num_slaves, modules,
    key_name, identity_file,
    instance_type,
    region,
    availability_zone="",
    ami="",
    spot_price=None,
    vpc_id="", subnet_id="", placement_group="",
    tenancy="default", ebs_optimized=False,
    instance_initiated_shutdown_behavior="stop"):
    connection = ec2.connect_to_region(region_name=region)
    def get_or_create_security_group(name, vpc_id):
        search_results = connection.get_all_security_groups(filters={"group-name": [name]})
        group = search_results[0] if search_results else None

        if not group:
            group = connection.create_security_group(
                name=name,
                description="flintrock-managed Spark group",
                vpc_id=vpc_id)

        try:
            # namedtuple('SecurityGroupRule', ['ip_protocol', 'from_port', 'to_port', 'src_group'])(ip_protocol, from_port, to_port, src_group)
            group.authorize(
                ip_protocol='icmp',
                from_port=-1,
                to_port=-1,
                src_group=group)
            group.authorize(
                ip_protocol='tcp',
                from_port=0,
                to_port=65535,
                src_group=group)
            group.authorize(
                ip_protocol='udp',
                from_port=0,
                to_port=65535,
                src_group=group)
        except Exception as e:
            print("ERROR: Make a clean way to check for existing SG rules.")

        flintrock_client_ip = urllib.request.urlopen('http://checkip.amazonaws.com/').read().decode('utf-8').strip()
        flintrock_client_cidr = '{ip}/32'.format(ip=flintrock_client_ip)

        # ssh_rule = None

        # for rule in group.rules:
        #     if int(rule.from_port) == 22 and int(rule.to_port) == 22:
        #         ssh_rule = rule
        #         break

        # if not ssh_rule or flintrock_client_cidr not in [str(grant) for grant in ssh_rule.grants]:
        try:
            group.authorize(
                ip_protocol='tcp',
                from_port=22,
                to_port=22,
                cidr_ip=flintrock_client_cidr)
            group.authorize(
                ip_protocol='tcp',
                from_port=8080,
                to_port=8081,
                cidr_ip=flintrock_client_cidr)
            group.authorize(
                ip_protocol='tcp',
                from_port=4040,
                to_port=4040,
                cidr_ip=flintrock_client_cidr)
        except Exception as e:
            print("ERROR: Really, make it easy to manipulate SG rules.")

        return group
    security_group = get_or_create_security_group(name=cluster_name, vpc_id=vpc_id)
    reservation = connection.run_instances(
        image_id=ami,
        min_count=(num_slaves + 1),
        max_count=(num_slaves + 1),
        key_name=key_name,
        instance_type=instance_type,
        placement=availability_zone,
        security_group_ids=[security_group.id],
        subnet_id=subnet_id,
        placement_group=placement_group,
        tenancy=tenancy,
        ebs_optimized=ebs_optimized,
        instance_initiated_shutdown_behavior=instance_initiated_shutdown_behavior)

    master_instance = reservation.instances[0]
    slave_instances = reservation.instances[1:]

    cluster_key_pair = generate_ssh_key_pair()

    time.sleep(10)  # Eventual consistency complexity tax.

    try:
        # TODO: Abstract away. No-one wants to see this async shite here.
        loop = asyncio.get_event_loop()

        tasks = []
        for instance in reservation.instances:
            task = loop.run_in_executor(
                executor=None,
                callback=functools.partial(
                    provision_ec2_node,
                    modules=modules,
                    region=region,
                    instance=instance,
                    identity_file=identity_file,
                    cluster_key_pair=cluster_key_pair,
                    master_instance=master_instance))
            tasks.append(task)
        loop.run_until_complete(asyncio.wait(tasks))

        loop.close()

        # --- Spark: Configure master. ---
        # TODO: Move to step that runs once all nodes are known to be "ready".
        with paramiko.client.SSHClient() as client:
            client.load_system_host_keys()
            client.set_missing_host_key_policy(paramiko.client.AutoAddPolicy())
            client.connect(
                username="ec2-user",
                hostname=master_instance.public_dns_name,
                key_filename=identity_file,
                timeout=3)

            for i in [master_instance] + slave_instances:
                i.update()

            spark = Spark(
                version="1.3.0",
                ssh_client=client,
                master_hostname=master_instance.public_dns_name,
                slave_hostnames=[s.public_dns_name for s in slave_instances])
            spark.configure_master()

        # TODO: Move to master_login() method.
        ret = subprocess.call(
            """
            set -x
            ssh -o "StrictHostKeyChecking=no" -i {identity_file} ec2-user@{host}
            """.format(
                identity_file=shlex.quote(identity_file),
                host=shlex.quote(master_instance.public_dns_name)),
            shell=True)
        # print("Shell returned: {r}".format(r=ret))
    except KeyboardInterrupt as e:
        print("Exiting...")
        sys.exit(1)
    finally:
        for instance in reservation.instances:
            instance.terminate()


# boto is not thread-safe so each task needs to create its own connection.
# Reference, from boto's primary maintainer: http://stackoverflow.com/a/19542645/
# TODO: Pass in cluster_info object instead of key_pair and master_hostname.
def provision_ec2_node(
    modules,
    region,
    instance,
    identity_file,
    cluster_key_pair,
    master_instance):
    connection = ec2.connect_to_region(region_name=region)

    while True:
        instance.update()
        if instance.state != 'running':
            # print("State is {s}. Sleeping on {i}...".format(s=instance.state, i=instance.id))
            time.sleep(5)
        else:
            # print("Instance {i} is running.".format(i=instance.id))
            break

    with paramiko.client.SSHClient() as client:
        client.load_system_host_keys()
        client.set_missing_host_key_policy(paramiko.client.AutoAddPolicy())

        while True:
            try:
                client.connect(
                    username="ec2-user",
                    hostname=instance.public_dns_name,
                    key_filename=identity_file,
                    timeout=3)
                print("[{h}] SSH online.".format(h=instance.public_dns_name))
                break
            except socket.timeout as e:
                # print("[{h}] SSH timed out.".format(h=instance.public_dns_name))
                # print(e)
                time.sleep(5)
            except socket.error as e:
                if e.errno != 61:
                    print(e)
                    raise
                # print("[{h}] SSH refused.".format(h=instance.public_dns_name))
                # print(e)
                time.sleep(5)

        # --- SSH is now available. ---
        stdin, stdout, stderr = client.exec_command(command="""
                echo {private_key} > ~/.ssh/id_rsa
                chmod 400 ~/.ssh/id_rsa
            """.format(private_key=shlex.quote(cluster_key_pair.private)))
        exit_status = stdout.channel.recv_exit_status()
        if exit_status:
            error_output = stderr.read().decode("utf8").rstrip('\n')
            raise Exception(error_output)

        stdin, stdout, stderr = client.exec_command(
            command='echo {public_key} >> ~/.ssh/authorized_keys'.format(
                public_key=shlex.quote(cluster_key_pair.public)))
        exit_status = stdout.channel.recv_exit_status()
        if exit_status:
            error_output = stderr.read().decode("utf8").rstrip('\n')
            raise Exception(error_output)

        # --- Install Spark. ---
        master_instance.update()  # TODO: Get rid of this whole shite.
        spark = Spark(
            version="1.3.0",
            ssh_client=client,
            master_hostname=master_instance.public_dns_name)
        spark.install()


def ssh_command_output(client: "paramiko.client.SSHClient", command):
    stdin, stdout, stderr = client.exec_command(command, get_pty=True)
    exit_status = stdout.channel.recv_exit_status()
    if exit_status:
        raise Exception(stderr.read().decode("utf8"))

    return stdout.read().decode("utf8").rstrip('\n')


def destroy(provider, cluster_name, provider_options, assume_yes=False):
    pass


def destroy_ec2(cluster_name, assume_yes=False,
    delete_groups=False):
    pass


def add_slaves(provider, cluster_name, num_slaves, provider_options):
    pass


def add_slaves_ec2(cluster_name, num_slaves,
    identity_file):
    pass


def remove_slaves(provider, cluster_name, num_slaves, provider_options, assume_yes=False):
    pass


def remove_slaves_ec2(cluster_name, num_slaves, assume_yes=False):
    pass


def describe(provider, cluster_name, provider_options, master_hostname_only=False):
    pass


def describe_ec2(cluster_name, master_hostname_only=False):
    pass


def login(provider, cluster_name, provider_options):
    pass


def login_ec2(cluster_name):
    pass


def start(provider, cluster_name, provider_options):
    pass


def start_ec2(cluster_name):
    pass


def stop(provider, cluster_name, provider_options, assume_yes=False):
    pass


def stop_ec2(cluster_name, assume_yes=False):
    pass


if __name__ == "__main__":
    # sys.exit()
    launch(
        provider="ec2",
        cluster_name="flintrock-test",
        num_slaves=2,
        modules=None,
        provider_options={
            "key_name": "nick",
            "identity_file": "/Users/nicholaschammas/.ssh/nick.pem",
            "instance_type": "m3.medium",
            "region": "us-east-1",
            "ami": "ami-146e2a7c"
        })

# print(json.dumps(env, indent=True))
# print(json.dumps(fabric.state.commands, indent=True))
