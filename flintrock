#!/usr/bin/env python3

"""
flintrock

Nothing here should be distribution-specific (e.g. yum vs. apt-get).
That stuff belongs under image-build/.

Major TODOs:
  * Specifying Spark version at command line / config file
  * Command-line options
    - login
    - destroy
  * Module reorg - EC2 stuff to its own module
  * EBS volume setup (maybe replace with EFS?)
"""

import os
import sys
import shlex
import subprocess
import pprint
import click
import asyncio
import functools
import boto
import boto.ec2 as ec2
import paramiko
import socket
import json
import yaml
import time
import urllib.request
import tempfile
import textwrap
from datetime import datetime
from collections import namedtuple

_SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))


def timeit(func):
    def wrapper(*args, **kwargs):
        start = datetime.now().replace(microsecond=0)
        res = func(*args, **kwargs)
        end = datetime.now().replace(microsecond=0)
        print("{f} finished in {t}.".format(f=func.__name__, t=(end - start)))
        return res
    return wrapper


def generate_ssh_key_pair() -> namedtuple('KeyPair', ['public', 'private']):
    """
    Generate an SSH key pair that the cluster can use for intra-cluster
    communication.
    """
    with tempfile.TemporaryDirectory() as tempdir:
        ret = subprocess.check_call(
            """
            ssh-keygen -q -t rsa -N '' -f {key_file} -C flintrock
            """.format(
                key_file=shlex.quote(tempdir + "/flintrock_rsa")),
            shell=True)

        with open(file=tempdir + "/flintrock_rsa") as private_key_file:
            private_key = private_key_file.read()

        with open(file=tempdir + "/flintrock_rsa.pub") as public_key_file:
            public_key = public_key_file.read()

    return namedtuple('KeyPair', ['public', 'private'])(public_key, private_key)


ClusterInfo = namedtuple(
    'ClusterInfo', [
        'key_pair',
        'master_host',
        'slave_hosts',
        'spark_scratch_dir',
        'spark_master_opts'
    ])


# TODO: Cache these files. (?) They are being read potentially tens or
#       hundreds of times. Maybe it doesn't matter because the files 
#       are so small.
# NOTE: functools.lru_cache() doesn't work here because the mapping is
#       not hashable.
def get_formatted_template(path: str, mapping: dict) -> str:
    class TemplateDict(dict):
        def __missing__(self, key):
            return '{' + key + '}'

    with open(path) as f:
        formatted = f.read().format_map(TemplateDict(**mapping))

    return formatted


class Spark:
    def __init__(self, version):
        self.version = version

    def install(self,
        ssh_client: paramiko.client.SSHClient,
        cluster_info: ClusterInfo):
        """
        Downloads and installs Spark on a given node.
        """
        print("[{h}] Installing Spark...".format(
            h=ssh_client.get_transport().getpeername()[0]))

        # TODO: Figure out how these non-template paths should work.
        ssh_check_output(
            client=ssh_client,
            command="""
                set -e

                echo {f} > /tmp/install-spark.sh
                chmod 755 /tmp/install-spark.sh

                /tmp/install-spark.sh {v} hadoop1
            """.format(
                f=shlex.quote(
                    get_formatted_template(
                        path='./install-spark.sh',
                        mapping=vars(cluster_info))),
                v=shlex.quote(self.version)))

        template_path = "./spark/conf/spark-env.sh"
        ssh_check_output(
            client=ssh_client,
            command="""
                echo {f} > {p}
            """.format(
                f=shlex.quote(
                    get_formatted_template(
                        path="templates/" + template_path,
                        mapping=vars(cluster_info))),
                p=shlex.quote(template_path)))

    def configure(self):
        """
        Runs after all nodes are "ready".
        """
        pass

    def configure_master(self,
        ssh_client: paramiko.client.SSHClient,
        cluster_info: ClusterInfo):
        """
        Configures the Spark master and starts both the master and slaves.
        """
        host = ssh_client.get_transport().getpeername()[0]
        print("[{h}] Configuring Spark master...".format(h=host))

        ssh_check_output(
            client=ssh_client,
            command="""
                set -e

                echo {s} > spark/conf/slaves

                spark/sbin/start-master.sh

                set +e

                master_ui_response_code=0
                while [ "$master_ui_response_code" -ne 200 ]; do
                    sleep 1
                    master_ui_response_code="$(
                        curl --head --silent --output /dev/null \
                             --write-out "%{{http_code}}" {m}:8080
                    )"
                done

                set -e

                spark/sbin/start-slaves.sh
            """.format(
                s=shlex.quote('\n'.join(cluster_info.slave_hosts)),
                m=shlex.quote(cluster_info.master_host)))

        # Spark health check
        # TODO: Move to health_check() module method?
        time.sleep(30)

        spark_master_ui = 'http://{m}:8080/json/'.format(m=cluster_info.master_host)

        spark_ui_info = json.loads(
            urllib.request.urlopen(spark_master_ui).read().decode('utf-8'))

        print(textwrap.dedent(
            """\
            Spark Health Report:
              * Master: {status}
              * Workers: {workers}
              * Cores: {cores}
              * Memory: {memory:.1f} GB\
            """.format(
                status=spark_ui_info['status'],
                workers=len(spark_ui_info['workers']),
                cores=spark_ui_info['cores'],
                memory=spark_ui_info['memory'] / 1024)))        

    def configure_slave(self):
        pass


@click.group()
@click.option('--config', default=_SCRIPT_DIR + '/config.yaml')
@click.option('--provider', default='ec2', type=click.Choice(['ec2']))
@click.version_option(version='dev')  # TODO: Replace with setuptools auto-detect.
@click.pass_context
def cli(cli_context, config, provider):
    cli_context.obj['provider'] = provider

    with open(config) as f:
        config_map = normalize_keys(yaml.safe_load(f))

    cli_context.default_map = config_map


# @timeit  # Why doesn't this work?
@cli.command()
@click.argument('cluster-name')
@click.option('--num-slaves', type=int, required=True)
@click.option('--install-spark/--no-install-spark', default=True, show_default=True)
# TODO: Accept + auto-load Spark version, git repo (?), and other properties.
@click.option('--ec2-key-name', required=True)
@click.option('--ec2-identity-file', required=True, help="Path to .pem file for SSHing into nodes.")
@click.option('--ec2-instance-type', default='m3.medium', show_default=True)
@click.option('--ec2-region', default='us-east-1', show_default=True)
@click.option('--ec2-availability-zone')
@click.option('--ec2-ami')
@click.option('--ec2-spot-price', type=float)
@click.option('--ec2-vpc-id')
@click.option('--ec2-subnet-id')
@click.option('--ec2-placement-group')
@click.option('--ec2-tenancy', default='default')
@click.option('--ec2-ebs-optimized/--no-ec2-ebs-optimized', default=False)
@click.option('--ec2-instance-initiated-shutdown-behavior', default='stop')
@click.pass_context
def launch(cli_context,
    cluster_name, num_slaves,
    install_spark,
    ec2_key_name,
    ec2_identity_file,
    ec2_instance_type,
    ec2_region,
    ec2_availability_zone,
    ec2_ami,
    ec2_spot_price,
    ec2_vpc_id,
    ec2_subnet_id,
    ec2_placement_group,
    ec2_tenancy,
    ec2_ebs_optimized,
    ec2_instance_initiated_shutdown_behavior):
    """
    Launch a cluster on the specified provider.
    
    EC2 is currently the only supported provider.
    """
    # print(cli_context.obj, install_spark)
    # sys.exit()

    modules = []

    if install_spark:
        spark = Spark(version='1.3.0')
        modules += [spark]

    if cli_context.obj['provider'] == 'ec2':
        return launch_ec2(
            cluster_name=cluster_name, num_slaves=num_slaves, modules=modules,
            key_name=ec2_key_name,
            identity_file=ec2_identity_file,
            instance_type=ec2_instance_type,
            region=ec2_region,
            availability_zone=ec2_availability_zone,
            ami=ec2_ami,
            spot_price=ec2_spot_price,
            vpc_id=ec2_vpc_id,
            subnet_id=ec2_subnet_id,
            placement_group=ec2_placement_group,
            tenancy=ec2_tenancy,
            ebs_optimized=ec2_ebs_optimized,
            instance_initiated_shutdown_behavior=ec2_instance_initiated_shutdown_behavior)
    else:
        raise Exception("This provider is not supported: {p}".format(p=provider))


# Move to ec2 module and call as ec2.launch()?
@timeit
def launch_ec2(cluster_name, num_slaves, modules,
    key_name, identity_file,
    instance_type,
    region,
    availability_zone="",
    ami="",
    spot_price=None,
    vpc_id="", subnet_id="", placement_group="",
    tenancy="default", ebs_optimized=False,
    instance_initiated_shutdown_behavior="stop"):
    """
    Launch a fully functional cluster on EC2 with the specific configuration
    and installed modules.
    """
    connection = ec2.connect_to_region(region_name=region)

    def get_or_create_security_group(name, vpc_id):
        """
        If one does not already exist, create a security with all the appropriate
        rules for the cluster to use.
        """
        search_results = connection.get_all_security_groups(filters={"group-name": [name]})
        group = search_results[0] if search_results else None

        if not group:
            group = connection.create_security_group(
                name=name,
                description="flintrock-managed group",
                vpc_id=vpc_id)

        SecurityGroupRule = namedtuple(
            'SecurityGroupRule', [
                'ip_protocol',
                'from_port',
                'to_port',
                'src_group',
                'cidr_ip'])

        # Rules for internal cluster communication.
        cluster_rules = [
            SecurityGroupRule(
                ip_protocol='icmp',
                from_port=-1,
                to_port=-1,
                src_group=group,
                cidr_ip=None),
            SecurityGroupRule(
                ip_protocol='tcp',
                from_port=0,
                to_port=65535,
                src_group=group,
                cidr_ip=None),
            SecurityGroupRule(
                ip_protocol='udp',
                from_port=0,
                to_port=65535,
                src_group=group,
                cidr_ip=None)
        ]

        for rule in cluster_rules:
            try:
                group.authorize(**vars(rule))
            except boto.exception.EC2ResponseError as e:
                if e.error_code != 'InvalidPermission.Duplicate':
                    print("Error adding rule: {r}".format(r=rule))
                    raise

        flintrock_client_ip = urllib.request.urlopen('http://checkip.amazonaws.com/').read().decode('utf-8').strip()
        flintrock_client_cidr = '{ip}/32'.format(ip=flintrock_client_ip)

        # Rules for the client interacting with the cluster.
        client_rules = [
            SecurityGroupRule(
                ip_protocol='tcp',
                from_port=22,
                to_port=22,
                cidr_ip=flintrock_client_cidr,
                src_group=None),
            SecurityGroupRule(
                ip_protocol='tcp',
                from_port=8080,
                to_port=8081,
                cidr_ip=flintrock_client_cidr,
                src_group=None),
            SecurityGroupRule(
                ip_protocol='tcp',
                from_port=4040,
                to_port=4040,
                cidr_ip=flintrock_client_cidr,
                src_group=None)
        ]

        # TODO: Don't try adding rules that already exist.
        for rule in client_rules:
            try:
                group.authorize(**vars(rule))
            except boto.exception.EC2ResponseError as e:
                if e.error_code != 'InvalidPermission.Duplicate':
                    print("Error adding rule: {r}".format(r=rule))
                    raise

        return group

    security_group = get_or_create_security_group(name=cluster_name, vpc_id=vpc_id)

    try:
        reservation = connection.run_instances(
            image_id=ami,
            min_count=(num_slaves + 1),
            max_count=(num_slaves + 1),
            key_name=key_name,
            instance_type=instance_type,
            placement=availability_zone,
            security_group_ids=[security_group.id],
            subnet_id=subnet_id,
            placement_group=placement_group,
            tenancy=tenancy,
            ebs_optimized=ebs_optimized,
            instance_initiated_shutdown_behavior=instance_initiated_shutdown_behavior)

        time.sleep(10)  # AWS metadata eventual consistency tax.

        while True:
            for instance in reservation.instances:
                if instance.state == 'running':
                    continue
                else:
                    instance.update()
                    time.sleep(3)
                    break
            else:
                print("All {c} instances now running.".format(
                    c=len(reservation.instances)))
                break

        master_instance = reservation.instances[0]
        slave_instances = reservation.instances[1:]

        cluster_info = ClusterInfo(
            key_pair=generate_ssh_key_pair(),
            master_host=master_instance.public_dns_name,
            slave_hosts=[instance.public_dns_name for instance in slave_instances],
            spark_scratch_dir='/mnt/spark',
            spark_master_opts="")

        # TODO: Abstract away. No-one wants to see this async shite here.
        loop = asyncio.get_event_loop()

        tasks = []
        for instance in reservation.instances:
            task = loop.run_in_executor(
                executor=None,
                callback=functools.partial(
                    provision_ec2_node,
                    modules=modules,
                    host=instance.ip_address,
                    identity_file=identity_file,
                    cluster_info=cluster_info))
            tasks.append(task)
        loop.run_until_complete(asyncio.wait(tasks))
        loop.close()

        print("All {c} instances provisioned.".format(
            c=len(reservation.instances)))

        # --- This stuff here runs after all the nodes are provisioned. ---
        with paramiko.client.SSHClient() as client:
            client.load_system_host_keys()
            client.set_missing_host_key_policy(paramiko.client.AutoAddPolicy())

            client.connect(
                username="ec2-user",
                hostname=master_instance.public_dns_name,
                key_filename=identity_file,
                timeout=3)

            for module in modules:
                module.configure_master(
                    ssh_client=client,
                    cluster_info=cluster_info)

            # Login to the master for manual inspection.
            # TODO: Move to master_login() method.
            # ret = subprocess.call(
            #     """
            #     set -x
            #     ssh -o "StrictHostKeyChecking=no" \
            #         -i {identity_file} \
            #         ec2-user@{host}
            #     """.format(
            #         identity_file=shlex.quote(identity_file),
            #         host=shlex.quote(master_instance.public_dns_name)),
            #     shell=True)
            # print("Shell returned: {r}".format(r=ret))

    except KeyboardInterrupt as e:
        print("Exiting...")
        sys.exit(1)
    finally:
        print("Terminating all {c} instances...".format(
            c=len(reservation.instances)))

        for instance in reservation.instances:
            instance.terminate()


# boto is not thread-safe so each task needs to create its own connection.
# Reference, from boto's primary maintainer: http://stackoverflow.com/a/19542645/
def provision_ec2_node(
    modules,
    host,
    identity_file,
    cluster_info):
    """
    Connect to a freshly launched EC2 instance, set it up for SSH access, and
    install the specified modules.
    
    This function is intended to be called on all cluster nodes in parallel.
    
    No master- or slave-specific logic should be in this method.
    """
    with paramiko.client.SSHClient() as client:
        client.load_system_host_keys()
        client.set_missing_host_key_policy(paramiko.client.AutoAddPolicy())

        while True:
            try:
                client.connect(
                    username="ec2-user",
                    hostname=host,
                    key_filename=identity_file,
                    timeout=3)
                print("[{h}] SSH online.".format(h=host))
                break
            except socket.timeout as e:
                time.sleep(5)
            except socket.error as e:
                if e.errno != 61:
                    raise
                time.sleep(5)

        # --- SSH is now available. ---
        ssh_check_output(
            client=client,
            command="""
                set -e

                echo {private_key} > ~/.ssh/id_rsa
                echo {public_key} >> ~/.ssh/authorized_keys

                chmod 400 ~/.ssh/id_rsa
            """.format(
                private_key=shlex.quote(cluster_info.key_pair.private),
                public_key=shlex.quote(cluster_info.key_pair.public)))

        # --- Install Modules. ---
        for module in modules:
            module.install(
                ssh_client=client,
                cluster_info=cluster_info)


def ssh_check_output(client: "paramiko.client.SSHClient", command: str):
    """
    Run a command via the provided SSH client and return the output captured
    on stdout.
    
    Raise an exception if the command returns a non-zero code.
    """
    stdin, stdout, stderr = client.exec_command(command, get_pty=True)
    exit_status = stdout.channel.recv_exit_status()

    if exit_status:
        # TODO: Return a custom exception that includes the return code.
        # See: https://docs.python.org/3/library/subprocess.html#subprocess.check_output
        raise Exception(stderr.read().decode("utf8").rstrip('\n'))

    return stdout.read().decode("utf8").rstrip('\n')


def destroy(provider, cluster_name, provider_options, assume_yes=False):
    pass


def destroy_ec2(cluster_name, assume_yes=False,
    delete_groups=False):
    pass


def add_slaves(provider, cluster_name, num_slaves, provider_options):
    pass


def add_slaves_ec2(cluster_name, num_slaves,
    identity_file):
    pass


def remove_slaves(provider, cluster_name, num_slaves, provider_options, assume_yes=False):
    pass


def remove_slaves_ec2(cluster_name, num_slaves, assume_yes=False):
    pass


def describe(provider, cluster_name, provider_options, master_hostname_only=False):
    pass


def describe_ec2(cluster_name, master_hostname_only=False):
    pass


def login(provider, cluster_name, provider_options):
    pass


def login_ec2(cluster_name):
    pass


def start(provider, cluster_name, provider_options):
    pass


def start_ec2(cluster_name):
    pass


def stop(provider, cluster_name, provider_options, assume_yes=False):
    pass


def stop_ec2(cluster_name, assume_yes=False):
    pass


def normalize_keys(obj):
    """
    Used to map keys from config files to Python parameter names.
    """
    if type(obj) != dict:
        return obj
    else:
        return { k.replace('-', '_'): normalize_keys(v) for k,v in obj.items() }


if __name__ == "__main__":
    cli(obj={})
