#!/usr/bin/env python -Wdefault
from __future__ import print_function


import concurrent.futures
import boto.ec2 as ec2
import json
import yaml
import time

# import fabric.state
# from fabric.api import task, local, parallel, run, env
# from fabric.tasks import execute

# env.colorize_errors = True
# env.disable_known_hosts = True
# env.eagerly_disconnect = True
# env.keepalive = 30
# env.key_filename = "~/.ssh/nick.pem"
# env.rcfile = ""
# env.hosts = ['localhost', '127.0.0.1']

SUPPORTED_MODULES = {
    "spark": {
        "version": "1.3.0",
        "git-repo": "https://github.com/apache/spark"
    }
}


# @parallel
# def date():
#     run("date")


def launch(provider, cluster_name, num_slaves, modules, provider_options):
    if provider == 'ec2':
        return launch_ec2(
            cluster_name=cluster_name,
            num_slaves=num_slaves,
            modules=modules,
            **provider_options)
    pass


# Move to ec2 module and call as ec2.launch()?
def launch_ec2(cluster_name, num_slaves, modules,
    key_name, identity_file,
    instance_type,
    region,
    availability_zone="",
    ami="",
    spot_price=None,
    vpc_id="", subnet_id="", placement_group="",
    tenancy="default", ebs_optimized=False,
    instance_initiated_shutdown_behavior="stop"):
    connection = ec2.connect_to_region(region_name=region)
    def get_or_create_security_group(name, vpc_id):
        search_results = connection.get_all_security_groups(filters={"group-name": [name]})
        group = search_results[0] if search_results else None
        if not group:
            group = connection.create_security_group(
                name=name,
                description="flintrock-managed Spark group",
                vpc_id=vpc_id)
        return group
    security_group = get_or_create_security_group(name=cluster_name, vpc_id=vpc_id)
    reservation = connection.run_instances(
        image_id=ami,
        min_count=(num_slaves + 1),
        max_count=(num_slaves + 1),
        key_name=key_name,
        instance_type=instance_type,
        placement=availability_zone,
        security_group_ids=[security_group.id],
        subnet_id=subnet_id,
        placement_group=placement_group,
        tenancy=tenancy,
        ebs_optimized=ebs_optimized,
        instance_initiated_shutdown_behavior=instance_initiated_shutdown_behavior)

    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:
        instance_futures = {executor.submit(provision_ec2_node, modules=None, region=region, instance=i, identity_file=None): i for i in reservation.instances}
        try:
            print("Waiting for instances to come up...")
            for instance_future in concurrent.futures.as_completed(instance_futures, timeout=1000):
                instance_future.result()
        except KeyboardInterrupt:
            print("Received interrupt. Cancelling futures...")
            # TODO: Something
    # TODO: configure_master
    pass


# boto is not thread-safe so each task needs to create its own connection.
# Reference, from boto's primary maintainer: http://stackoverflow.com/a/19542645/
def provision_ec2_node(modules,
    region,
    instance,
    identity_file):
    connection = ec2.connect_to_region(region_name=region)
    while True:
        if instance.state != 'running':
            print("State is {s}. Sleeping on {i}...".format(s=instance.state, i=instance.id))
            time.sleep(5)
        else:
            print("Instance {i} is running.".format(i=instance.id))
            break


def destroy(provider, cluster_name, provider_options, assume_yes=False):
    pass


def destroy_ec2(cluster_name, assume_yes=False,
    delete_groups=False):
    pass


def add_slaves(provider, cluster_name, num_slaves, provider_options):
    pass


def add_slaves_ec2(cluster_name, num_slaves,
    identity_file):
    pass


def remove_slaves(provider, cluster_name, num_slaves, provider_options, assume_yes=False):
    pass


def remove_slaves_ec2(cluster_name, num_slaves, assume_yes=False):
    pass


def describe(provider, cluster_name, provider_options, master_hostname_only=False):
    pass


def describe_ec2(cluster_name, master_hostname_only=False):
    pass


def login(provider, cluster_name, provider_options):
    pass


def login_ec2(cluster_name):
    pass


def start(provider, cluster_name, provider_options):
    pass


def start_ec2(cluster_name):
    pass


def stop(provider, cluster_name, provider_options, assume_yes=False):
    pass


def stop_ec2(cluster_name, assume_yes=False):
    pass


if __name__ == "__main__":
    launch(
        provider="ec2",
        cluster_name="flintrock-test",
        num_slaves=1,
        modules=None,
        provider_options={
            "key_name": "nick",
            "identity_file": "/Users/nicholaschammas/.ssh/nick.pem",
            "instance_type": "m3.medium",
            "region": "us-east-1",
            "ami": "ami-146e2a7c"
        })

# print(json.dumps(env, indent=True))
# print(json.dumps(fabric.state.commands, indent=True))
